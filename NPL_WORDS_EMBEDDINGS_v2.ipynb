{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuYbwms2mnkB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import urllib.request\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs\n",
        "import nltk\n",
        "# Descargar el diccionario\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import sys\n",
        "#import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_PieaU7r0DC"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
        "# Sobracargamos el callback para poder tener esta información\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss"
      ],
      "metadata": {
        "id": "GX5inh72sgrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIQAQkattYk1"
      },
      "outputs": [],
      "source": [
        "def perform_lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z7Dpzw6tGea"
      },
      "outputs": [],
      "source": [
        "def get_processed_text(document):\n",
        "    # 1 - reduce el texto a mínuscula\n",
        "    # 2 - quitar los simbolos de puntuacion\n",
        "    # 3 - realiza la tokenización\n",
        "    # 4 - realiza la lematización\n",
        "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxiQoFAktl2b",
        "outputId": "87687095-cb82-4977-a502-ad8668b981cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtvZ6Ii5zl3f"
      },
      "source": [
        "##TOPICS:\n",
        "* HOT DOG\n",
        "* FAST FOOD\n",
        "* COCA COLA\n",
        "* FOOD\n",
        "* OBESITY\n",
        "* OVERWEIGHT\n",
        "* SEDENTARY LIFESTYLE\n",
        "\n",
        "##WE ONLY SELECT 3 TOPICS RANDOMLY, IT CHANGES IN EACH EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgp6_YH2nUix"
      },
      "outputs": [],
      "source": [
        "topics = [\"https://en.wikipedia.org/wiki/Hot_dog\",\"https://en.wikipedia.org/wiki/Fast_food\",\"https://en.wikipedia.org/wiki/Coca-Cola\",\"https://en.wikipedia.org/wiki/Food\",\"https://en.wikipedia.org/wiki/Obesity\",\"https://en.wikipedia.org/wiki/Overweight\",\"https://en.wikipedia.org/wiki/Sedentary_lifestyle\"]\n",
        "\n",
        "full_article = \"\"\n",
        "\n",
        "for i in topics:\n",
        "  raw_html = urllib.request.urlopen(i)\n",
        "  raw_html = raw_html.read()\n",
        "\n",
        "  article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "  article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "  article_text = ''\n",
        "\n",
        "  for para in article_paragraphs:\n",
        "      article_text += para.text\n",
        "\n",
        "  article_text = article_text.lower()\n",
        "  full_article = full_article + article_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbu5CCKanXXc"
      },
      "outputs": [],
      "source": [
        "text = re.sub(r'\\[[0-9]*\\]', ' ', full_article)\n",
        "text = re.sub(r'\\s+', ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1HfnBQXnZsO"
      },
      "outputs": [],
      "source": [
        "corpus = nltk.sent_tokenize(text)\n",
        "words = nltk.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "option = 0"
      ],
      "metadata": {
        "id": "2dukMz0GvqA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_1 = [s.split() for s in corpus]\n",
        "\n",
        "corpus_2 = []\n",
        "for t in corpus:\n",
        "    corpus_2.append(text_to_word_sequence(t))"
      ],
      "metadata": {
        "id": "Fvn2B93B2rPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crearmos el modelo generador de vectoeres\n",
        "\n",
        "w2v_model_1 = Word2Vec(\n",
        "                     min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
        "                     window=2,       # cant de palabras antes y desp de la predicha\n",
        "                     size=300,       # dimensionalidad de los vectores \n",
        "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
        "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
        "                     sg=0)           # modelo 0:CBOW  1:skipgram\n",
        "\n",
        "\n",
        "w2v_model_2 = Word2Vec(\n",
        "                     min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
        "                     window=2,       # cant de palabras antes y desp de la predicha\n",
        "                     size=300,       # dimensionalidad de los vectores \n",
        "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
        "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
        "                     sg=0)           # modelo 0:CBOW  1:skipgram"
      ],
      "metadata": {
        "id": "qWO4R-C-r6Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model_1.build_vocab(corpus_1)\n",
        "w2v_model_2.build_vocab(corpus_2)"
      ],
      "metadata": {
        "id": "ajYiPgL0sobB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cantidad de docs en el corpus:\", w2v_model_1.corpus_count)\n",
        "print(\"Cantidad de docs en el corpus:\", w2v_model_2.corpus_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvojO6STstIA",
        "outputId": "51988b0c-23f4-4c8e-fa0d-87806ca9d3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de docs en el corpus: 1424\n",
            "Cantidad de docs en el corpus: 1424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo generador de vectores\n",
        "# Utilizamos nuestro callback\n",
        "w2v_model_1.train(corpus,\n",
        "                 total_examples=w2v_model_1.corpus_count,\n",
        "                 epochs=20,\n",
        "                 compute_loss = True,\n",
        "                 callbacks=[callback()]\n",
        "                 )\n",
        "\n",
        "w2v_model_2.train(corpus,\n",
        "                 total_examples=w2v_model_2.corpus_count,\n",
        "                 epochs=20,\n",
        "                 compute_loss = True,\n",
        "                 callbacks=[callback()]\n",
        "                 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FbB6vu3sxQu",
        "outputId": "d4ef686a-8ea4-44b1-8245-7d0c346dd901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 14816.3486328125\n",
            "Loss after epoch 1: 6278.2314453125\n",
            "Loss after epoch 2: 6865.732421875\n",
            "Loss after epoch 3: 6166.09375\n",
            "Loss after epoch 4: 6537.875\n",
            "Loss after epoch 5: 6025.203125\n",
            "Loss after epoch 6: 6166.60546875\n",
            "Loss after epoch 7: 6086.3671875\n",
            "Loss after epoch 8: 6051.05078125\n",
            "Loss after epoch 9: 5993.875\n",
            "Loss after epoch 10: 6064.4296875\n",
            "Loss after epoch 11: 6259.40625\n",
            "Loss after epoch 12: 6250.5\n",
            "Loss after epoch 13: 6435.40625\n",
            "Loss after epoch 14: 6447.8125\n",
            "Loss after epoch 15: 5837.9921875\n",
            "Loss after epoch 16: 6436.6953125\n",
            "Loss after epoch 17: 5890.8359375\n",
            "Loss after epoch 18: 6133.9453125\n",
            "Loss after epoch 19: 5695.953125\n",
            "Loss after epoch 0: 55346.44140625\n",
            "Loss after epoch 1: 35086.37890625\n",
            "Loss after epoch 2: 33515.5703125\n",
            "Loss after epoch 3: 32798.265625\n",
            "Loss after epoch 4: 32816.03125\n",
            "Loss after epoch 5: 32888.734375\n",
            "Loss after epoch 6: 32213.734375\n",
            "Loss after epoch 7: 32076.03125\n",
            "Loss after epoch 8: 31907.6875\n",
            "Loss after epoch 9: 31624.78125\n",
            "Loss after epoch 10: 31805.0\n",
            "Loss after epoch 11: 31636.34375\n",
            "Loss after epoch 12: 32133.3125\n",
            "Loss after epoch 13: 31958.75\n",
            "Loss after epoch 14: 31978.15625\n",
            "Loss after epoch 15: 31834.96875\n",
            "Loss after epoch 16: 31706.1875\n",
            "Loss after epoch 17: 31388.875\n",
            "Loss after epoch 18: 31638.0\n",
            "Loss after epoch 19: 31337.75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1557806, 4092780)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##APARCIÓN DEL BMI, BODY MASS INDEX"
      ],
      "metadata": {
        "id": "IzWyWLRN5e8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_p = [\"healthy\"]"
      ],
      "metadata": {
        "id": "n_d7Pfkm4AgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model_1.wv.most_similar(positive=word_p,topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI4MLcK73NZ1",
        "outputId": "318a2439-b120-401b-9a6d-50849177d147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('well', 0.14870218932628632),\n",
              " ('located', 0.14741304516792297),\n",
              " ('bmi,', 0.14218726754188538),\n",
              " ('includes', 0.14185893535614014),\n",
              " ('versions', 0.1393348127603531),\n",
              " ('or', 0.13589119911193848),\n",
              " ('account', 0.13399715721607208),\n",
              " ('consumption', 0.1325126439332962),\n",
              " ('worldwide', 0.13174983859062195),\n",
              " ('\"new', 0.13106416165828705)]"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model_2.wv.most_similar(positive=word_p,topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQZdGK22s3Gq",
        "outputId": "9d0aaacd-6752-44d3-9ee6-ab66a19b228b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spending', 0.1551455855369568),\n",
              " ('well', 0.14870218932628632),\n",
              " ('located', 0.14741304516792297),\n",
              " ('includes', 0.14185893535614014),\n",
              " ('versions', 0.1393348127603531),\n",
              " ('2005', 0.138925239443779),\n",
              " ('or', 0.13589119911193848),\n",
              " ('account', 0.13399715721607208),\n",
              " ('consumption', 0.1325126439332962),\n",
              " ('worldwide', 0.13174983859062195)]"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NPL_WORDS_EMBEDDINGS_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}