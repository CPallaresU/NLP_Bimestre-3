{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_BOT_TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "BuYbwms2mnkB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import urllib.request\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs\n",
        "import nltk\n",
        "# Descargar el diccionario\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import sys\n",
        "#import gradio as gr\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m pip install gradio --quiet"
      ],
      "metadata": {
        "id": "8_PieaU7r0DC"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OWN SIMILARITY"
      ],
      "metadata": {
        "id": "6-yeouNOuCAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"
      ],
      "metadata": {
        "id": "LKPeSH22sINd"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding (corpus):\n",
        "    \n",
        "    words_set = {}\n",
        "    words_set = set(words_set)\n",
        "    \n",
        "    for k in corpus:\n",
        "        \n",
        "        words = set(k.split(\" \"))\n",
        "        \n",
        "        for i in words :\n",
        "            \n",
        "            words_set.add(i)\n",
        "        \n",
        "    \n",
        "    one_h_encod = np.zeros((len(corpus),len(words_set)))\n",
        "    \n",
        "    tf_ = np.zeros((len(corpus),len(words_set)))\n",
        "    \n",
        "    words_list = list(words_set)\n",
        "    \n",
        "    \n",
        "    for q in range(0,len(corpus)):\n",
        "        \n",
        "        \n",
        "        words_  = list(corpus[q].split(\" \"))\n",
        "        \n",
        "        for h in words_:\n",
        "                    \n",
        "            one_h_encod [q][words_list.index(h)] = 1\n",
        "            tf_[q][words_list.index(h)] = tf_[q][words_list.index(h)] + 1\n",
        "    \n",
        "    return pd.DataFrame(one_h_encod,columns = words_list) , pd.DataFrame(tf_ , columns = words_list)"
      ],
      "metadata": {
        "id": "Oabwb_Yzrwuh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEACHER'S SIMILARITY"
      ],
      "metadata": {
        "id": "WmKiJl0VvBnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]"
      ],
      "metadata": {
        "id": "cIQAQkattYk1"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_processed_text(document):\n",
        "    # 1 - reduce el texto a mínuscula\n",
        "    # 2 - quitar los simbolos de puntuacion\n",
        "    # 3 - realiza la tokenización\n",
        "    # 4 - realiza la lematización\n",
        "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
      ],
      "metadata": {
        "id": "_z7Dpzw6tGea"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_input, corpus):\n",
        "    response = ''\n",
        "    # Sumar al corpus la pregunta del usuario para calcular\n",
        "    # su cercania con otros documentos/sentencias\n",
        "\n",
        "    corpus.append(user_input)\n",
        "    df_ohe, df_tf = one_hot_encoding(corpus)\n",
        "\n",
        "    ####### IDF #######\n",
        "\n",
        "    idf = np.log10(len(corpus) / df_ohe.sum(axis= 0 )) \n",
        "\n",
        "    df_idf = pd.DataFrame(idf).T\n",
        "\n",
        "    ####### IDF #######\n",
        "\n",
        "    ####### TF_IDF #######\n",
        "\n",
        "    tf_idf = df_tf.copy()\n",
        "\n",
        "    cont = 0\n",
        "\n",
        "    for j in df_tf.columns:\n",
        "    \n",
        "      tf_idf[j] = df_tf[j]*idf[cont]\n",
        "      cont = cont + 1 \n",
        "\n",
        "    ####### TF_IDF #######\n",
        "\n",
        "\n",
        "    ####### COSINE SIMILARITY #######\n",
        "\n",
        "    cosine_m = np.zeros((len(corpus),len(corpus)))\n",
        "\n",
        "    similitud = 0\n",
        "    mayor = 0\n",
        "    response = \" \"\n",
        "    for t in range(len(corpus)):\n",
        "                \n",
        "        a = np.array(tf_idf.iloc[t])\n",
        "        b = np.array(tf_idf.iloc[-1])\n",
        "\n",
        "        similitud =  (cosine_similarity(a,b))\n",
        "        if similitud >= mayor:\n",
        "          if user_input != corpus[t]:\n",
        "            mayor = similitud\n",
        "            response = corpus[t]\n",
        "          \n",
        "\n",
        "    ####### COSINE SIMILARITY #######\n",
        "    \n",
        "  \n",
        "    return response"
      ],
      "metadata": {
        "id": "yZI6eVFXsFqZ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "UxiQoFAktl2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7f9261-dce1-474a-e259-39aa4aa9a349"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOPICS:\n",
        "* HOT DOG\n",
        "* FAST FOOD\n",
        "* COCA COLA\n",
        "* FOOD\n",
        "* OBESITY\n",
        "* OVERWEIGHT\n",
        "* SEDENTARY LIFESTYLE\n",
        "\n",
        "##WE ONLY SELECT 3 TOPICS RANDOMLY, IT CHANGES IN EACH EXECUTION"
      ],
      "metadata": {
        "id": "KtvZ6Ii5zl3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics = [\"https://en.wikipedia.org/wiki/Hot_dog\",\"https://en.wikipedia.org/wiki/Fast_food\",\"https://en.wikipedia.org/wiki/Coca-Cola\",\"https://en.wikipedia.org/wiki/Food\",\"https://en.wikipedia.org/wiki/Obesity\",\"https://en.wikipedia.org/wiki/Overweight\",\"https://en.wikipedia.org/wiki/Sedentary_lifestyle\"]\n",
        "\n",
        "topics = random.sample(topics, 3)\n",
        "\n",
        "\n",
        "full_article = \"\"\n",
        "\n",
        "for i in topics:\n",
        "  raw_html = urllib.request.urlopen(i)\n",
        "  raw_html = raw_html.read()\n",
        "\n",
        "  article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "  article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "  article_text = ''\n",
        "\n",
        "  for para in article_paragraphs:\n",
        "      article_text += para.text\n",
        "\n",
        "  article_text = article_text.lower()\n",
        "  full_article = full_article + article_text"
      ],
      "metadata": {
        "id": "Tgp6_YH2nUix"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4clmCUA2jbL",
        "outputId": "a3b919ad-b71e-46da-9760-399fa8b13a1c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://en.wikipedia.org/wiki/Hot_dog', 'https://en.wikipedia.org/wiki/Coca-Cola', 'https://en.wikipedia.org/wiki/Obesity']\n",
            "['https://en.wikipedia.org/wiki/Food', 'https://en.wikipedia.org/wiki/Sedentary_lifestyle', 'https://en.wikipedia.org/wiki/Fast_food']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.sub(r'\\[[0-9]*\\]', ' ', full_article)\n",
        "text = re.sub(r'\\s+', ' ', text)"
      ],
      "metadata": {
        "id": "dbu5CCKanXXc"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = nltk.sent_tokenize(text)\n",
        "words = nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "y1HfnBQXnZsO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gradio as gr\n",
        "\n",
        "!{sys.executable} -m pip install gradio --quiet\n",
        "\n",
        "\n",
        "def bot_response(human_text):\n",
        "    print(\"Q:\", human_text)    \n",
        "    resp = generate_response(human_text.lower(), corpus)\n",
        "    print(\"A:\", resp)\n",
        "    return resp\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=bot_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\",\n",
        "    layout=\"vertical\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "ZNfAYyBpjxvN",
        "outputId": "fa33ff92-2c08-4dbb-c764-b3d7769f5e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://55672.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://55672.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: overweight\n",
            "A: a study done in the city of jeddah has shown that current fast-food habits are related to the increase of overweight and obesity among adolescents in saudi arabia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics"
      ],
      "metadata": {
        "id": "1Tojq_m12UDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}